
import os
import torch
import json
from dataclasses import dataclass, field
from typing import Optional, List, Dict, Any
from torch.utils.data import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
    DataCollatorForSeq2Seq,
    TrainerCallback
)
from peft import (
    LoraConfig,
    get_peft_model,
    TaskType,
    PeftModel
)
import pandas as pd
from sklearn.model_selection import train_test_split
import sacrebleu
from tqdm import tqdm


class BLEUCallback(TrainerCallback):
    """æ¯ä¸ª epoch ç»“æŸåè®¡ç®— BLEU åˆ†æ•°"""
    
    def __init__(self, val_df, tokenizer, lang_pair, max_length=256, sample_size=100):
        self.val_df = val_df
        self.tokenizer = tokenizer
        self.lang_pair = lang_pair
        self.max_length = max_length
        self.sample_size = sample_size  # é‡‡æ ·æ•°é‡ï¼Œé¿å…è¯„ä¼°å¤ªæ…¢
        
    def on_epoch_end(self, args, state, control, model=None, **kwargs):
        if model is None:
            return
            
        print(f"\nğŸ“Š Epoch {int(state.epoch)} ç»“æŸï¼Œè®¡ç®— BLEU...")
        
        # é‡‡æ ·éªŒè¯é›†
        sample_df = self.val_df.sample(n=min(self.sample_size, len(self.val_df)), random_state=42)
        
        target_lang = self.lang_pair.split('-')[1] if '-' in self.lang_pair else 'en'
        target_lang_name = {"en": "English", "ja": "Japanese", "zh": "Chinese"}.get(target_lang, "English")
        instruction = f"Please translate the following text into {target_lang_name}."
        
        predictions = []
        references = []
        
        model.eval()
        for idx, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc="Eval BLEU"):
            source = row['source']
            reference = row['target']
            
            messages = [
                {"role": "system", "content": "You are a professional legal translator."},
                {"role": "user", "content": f"{instruction}\n\n{source}"}
            ]
            text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            
            inputs = self.tokenizer([text], return_tensors="pt", truncation=True, max_length=self.max_length)
            inputs = {k: v.to(model.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                generated_ids = model.generate(
                    inputs['input_ids'],
                    attention_mask=inputs['attention_mask'],
                    max_new_tokens=self.max_length,
                    do_sample=False,
                    pad_token_id=self.tokenizer.pad_token_id
                )
            
            # åªå–ç”Ÿæˆçš„éƒ¨åˆ†
            new_tokens = generated_ids[0][inputs['input_ids'].shape[1]:]
            prediction = self.tokenizer.decode(new_tokens, skip_special_tokens=True)
            
            predictions.append(prediction)
            references.append(reference)
        
        # è®¡ç®— BLEU - æ ¹æ®ç›®æ ‡è¯­è¨€é€‰æ‹©åˆé€‚çš„ tokenizer
        # æ—¥è¯­ç”¨ 'ja-mecab'ï¼Œä¸­æ–‡ç”¨ 'zh'ï¼Œè‹±è¯­ç”¨é»˜è®¤ '13a'
        if target_lang == 'ja':
            bleu = sacrebleu.corpus_bleu(predictions, [references], tokenize='ja-mecab')
            tokenizer_name = 'ja-mecab'
        elif target_lang == 'zh':
            bleu = sacrebleu.corpus_bleu(predictions, [references], tokenize='zh')
            tokenizer_name = 'zh'
        else:
            bleu = sacrebleu.corpus_bleu(predictions, [references])
            tokenizer_name = '13a'
        print(f"âœ… Epoch {int(state.epoch)} Validation BLEU: {bleu.score:.2f} (tokenize={tokenizer_name})")
        
        model.train()

class QwenDataset(Dataset):
    def __init__(self, data, tokenizer, max_length=512, lang_pair='zh-ja'):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.lang_pair = lang_pair
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data.iloc[idx]
        source = item['source']
        target = item['target']
        
        # æ„å»º Prompt
        target_lang_name = "Japanese" if "ja" in self.lang_pair else "English"
        if self.lang_pair == "ja-zh" or self.lang_pair == "en-zh":
             target_lang_name = "Chinese"
             
        instruction = f"Please translate the following text into {target_lang_name}."
        
        # åˆ†åˆ«æ„å»º prompt éƒ¨åˆ†å’Œ response éƒ¨åˆ†
        messages_prompt = [
            {"role": "system", "content": "You are a professional legal translator."},
            {"role": "user", "content": f"{instruction}\n\n{source}"}
        ]
        
        # è·å– prompt éƒ¨åˆ†ï¼ˆä¸å« assistant å›å¤ï¼‰
        prompt_text = self.tokenizer.apply_chat_template(
            messages_prompt,
            tokenize=False,
            add_generation_prompt=True  # æ·»åŠ  assistant å¼€å§‹æ ‡è®°
        )
        
        # å®Œæ•´æ–‡æœ¬ï¼ˆå« assistant å›å¤ï¼‰
        messages_full = messages_prompt + [{"role": "assistant", "content": target}]
        full_text = self.tokenizer.apply_chat_template(
            messages_full,
            tokenize=False,
            add_generation_prompt=False
        )
        
        # ç¼–ç  prompt ä»¥è·å–é•¿åº¦
        prompt_ids = self.tokenizer(prompt_text, add_special_tokens=False).input_ids
        prompt_len = len(prompt_ids)
        
        # ç¼–ç å®Œæ•´æ–‡æœ¬
        encoding = self.tokenizer(
            full_text,
            max_length=self.max_length,
            padding="max_length",
            truncation=True
        )
        
        input_ids = encoding.input_ids
        attention_mask = encoding.attention_mask
        
        # å…³é”®ï¼šåªå¯¹ assistant å›å¤éƒ¨åˆ†è®¡ç®— lossï¼Œprompt éƒ¨åˆ†è®¾ä¸º -100
        labels = input_ids.copy()
        for i in range(min(prompt_len, len(labels))):
            labels[i] = -100  # å¿½ç•¥ prompt éƒ¨åˆ†çš„ loss
        
        # padding éƒ¨åˆ†ä¹Ÿè®¾ä¸º -100
        pad_token_id = self.tokenizer.pad_token_id
        for i in range(len(labels)):
            if input_ids[i] == pad_token_id:
                labels[i] = -100
        
        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels
        }

class QwenTrainer:
    def __init__(self, model_name="Qwen/Qwen3-4B-Instruct-2507", max_length=512, **kwargs):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model_name = model_name
        self.max_length = max_length
        print(f"Qwen åˆå§‹åŒ–: {model_name} on {self.device}")
        
        # åŠ è½½ Tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        if self.tokenizer.pad_token is None:
             self.tokenizer.pad_token = self.tokenizer.eos_token

    def load_data_from_json(self, train_json, test_json, lang_pair="zh-ja"):
        # å¤ç”¨ç®€å•çš„åŠ è½½é€»è¾‘
        def load(path):
            with open(path, 'r', encoding='utf-8') as f:
                return pd.DataFrame(json.load(f)['entries'])
        
        train_df = load(train_json)
        test_df = load(test_json)
        return train_df, test_df

    def train(self, datasets, output_dir="./qwen_finetuned", batch_size=4, num_epochs=3, learning_rate=2e-4, **kwargs):
        
        print("åŠ è½½ Qwen æ¨¡å‹ (bf16 ç²¾åº¦)...")
        model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        
        # é…ç½® LoRA - Qwen3 çš„ target_modulesï¼ˆå¢å¼ºç‰ˆï¼‰
        print("é…ç½® LoRA...")
        peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=16,  # å¢åŠ ç§©ï¼Œæå‡è¡¨è¾¾èƒ½åŠ›
            lora_alpha=32,
            lora_dropout=0.05,
            target_modules=[
                "q_proj", "k_proj", "v_proj", "o_proj",  # æ³¨æ„åŠ›å±‚
                "gate_proj", "up_proj", "down_proj"      # MLP å±‚ï¼ˆç¿»è¯‘ä»»åŠ¡é‡è¦ï¼‰
            ],
            bias="none"
        )
        
        model = get_peft_model(model, peft_config)
        model.print_trainable_parameters()
        
        # å‡†å¤‡æ•°æ®
        # è¿™é‡Œçš„ datasets æ˜¯ä» load_data_from_json è¿”å›çš„ DataFrame
        train_df = datasets['train']
        lang_pair = datasets.get('lang_pair', 'zh-ja')  # è·å–è¯­è¨€å¯¹
        
        # ç®€å•åˆ’åˆ†éªŒè¯é›†
        train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)
        
        train_ds = QwenDataset(train_df, self.tokenizer, self.max_length, lang_pair=lang_pair)
        val_ds = QwenDataset(val_df, self.tokenizer, self.max_length, lang_pair=lang_pair)
        
        training_args = TrainingArguments(
            output_dir=output_dir,
            per_device_train_batch_size=batch_size,
            gradient_accumulation_steps=4,
            learning_rate=learning_rate,
            num_train_epochs=num_epochs,
            logging_steps=10,
            save_strategy="epoch",
            eval_strategy="epoch",
            bf16=True,
            optim="adamw_torch"
        )
        
        # åˆ›å»º BLEU è¯„ä¼°å›è°ƒ
        bleu_callback = BLEUCallback(
            val_df=val_df, 
            tokenizer=self.tokenizer, 
            lang_pair=lang_pair,
            max_length=self.max_length,
            sample_size=100  # æ¯ epoch é‡‡æ · 100 æ¡è®¡ç®— BLEU
        )
        
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_ds,
            eval_dataset=val_ds,
            data_collator=DataCollatorForSeq2Seq(self.tokenizer, pad_to_multiple_of=8, return_tensors="pt", padding=True),
            callbacks=[bleu_callback]
        )
        
        print("å¼€å§‹è®­ç»ƒ Qwen...")
        trainer.train()
        
        print(f"ä¿å­˜ LoRA æƒé‡åˆ° {output_dir}/final")
        model.save_pretrained(f"{output_dir}/final")
        self.tokenizer.save_pretrained(f"{output_dir}/final")

    def translate(self, text, direction="zh2ja", max_length=512):
        # æ¨ç†é€»è¾‘
        # å¦‚æœæ˜¯åŠ è½½åçš„ LoRA æ¨¡å‹ï¼Œéœ€è¦å…ˆåŠ è½½åŸºåº§å†åŠ è½½ LoRA
        # è¿™é‡Œå‡è®¾ self.model å·²ç»åŠ è½½å¥½äº†ï¼ˆå¦‚æœæ˜¯åˆšåˆšè®­ç»ƒå®Œï¼‰
        # å¦‚æœæ˜¯é‡æ–°è¿è¡Œè„šæœ¬ï¼Œéœ€è¦ä¸“é—¨çš„åŠ è½½é€»è¾‘
        pass # åœ¨ main.py æˆ– test.py ä¸­å¤„ç†åŠ è½½

